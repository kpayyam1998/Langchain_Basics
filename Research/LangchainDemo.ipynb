{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "key=os.getenv(\"OPENAI_API_KEY_4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "key=os.getenv(\"OPENAI_API_KEY_4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.llms import OpenAI\n",
    "from langchain_openai import OpenAI\n",
    "#from langchain_community.llms import OpenAI\n",
    "\n",
    "llm=OpenAI()\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "#prompttemplate=PromptTemplate.from_template(\"Who is the president of {country} 2010?\") # country which we have to give\n",
    "prompttemplate = PromptTemplate(\n",
    "    input_variables=[\"country\"],\n",
    "    template=\"Who is the president of {country} 2010?\"  # We use this template to get the answer of the question.  # {country} will be replaced by the country name.  # This is how we create our prompt.  # We can add more variables as per our need.  # Here, we are only asking for president's name.  # For other questions, we will need to create different templates.  # We can also use LLM's capabilities to generate these templates dynamically.  # But for simplicity, we are using a fixed template.  # We will use this template to get the answer of the question.  # {country} will be replaced by the country name.  # This is how we create our prompt.  # We can add more variables as per our need.  # Here, we are only asking for president's name.  # For other questions,\n",
    ")\n",
    "\n",
    "# Smart methods\n",
    "chain = prompttemplate | llm\n",
    "\n",
    "chain.invoke({\n",
    "    \"country\": \"India\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe President of India in 2010 was Pratibha Patil.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Who is the president of india 2010?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# President identify in different country in the year of 2010\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "### PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "#prompttemplate=PromptTemplate.from_template(\"Who is the president of {country} 2010?\") # country which we have to give\n",
    "prompttemplate = PromptTemplate(\n",
    "    input_variables=[\"country\"],\n",
    "    template=\"Who is the president of {country} 2010?\"  # We use this template to get the answer of the question.  # {country} will be replaced by the country name.  # This is how we create our prompt.  # We can add more variables as per our need.  # Here, we are only asking for president's name.  # For other questions, we will need to create different templates.  # We can also use LLM's capabilities to generate these templates dynamically.  # But for simplicity, we are using a fixed template.  # We will use this template to get the answer of the question.  # {country} will be replaced by the country name.  # This is how we create our prompt.  # We can add more variables as per our need.  # Here, we are only asking for president's name.  # For other questions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who is the president of USA 2010?'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompttemplate.format(country=\"USA\") # We got the tempate we no need to give all the line in all time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chain which will combine together (llm and prompt templates) will give the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe president of India in 2010 was Pratibha Patil. '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Smart methods\n",
    "chain = prompttemplate | llm\n",
    "\n",
    "chain.invoke({\n",
    "    \"country\": \"India\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arung\\AppData\\Local\\Temp\\ipykernel_27168\\3878877598.py:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain=LLMChain(llm=llm,prompt=prompttemplate)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain=LLMChain(llm=llm,prompt=prompttemplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'country': 'USA',\n",
       " 'text': '\\n\\nThe president of the USA in 2010 was Barack Obama.'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"country\": \"USA\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RagLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
